{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion Mining & Sentiment Analysis\n",
    "\n",
    "## What is Opinion Mining?\n",
    "    Opinion Mining is related to mining human generated data. Humans are like sensors indicating their opinions when they use a particular product or watch a movie or use a service. \n",
    "    The output of these human generated sensors is unstructured data and may be in the form of video data, audio data, or text data. \n",
    "    Such an opinion mining can be subjective to one's perspective of analysis or interpretations of the unstructured data. Opinion provider is one person and who is interpreting the service/object experience and providing a feedback. Then there is one more person, the text miner/ data analyst who would be interpreting these opinions. Both the parties are working upon interpretations and sharing their understanding and inferences subjectively. This indicates that everything in opinion mining is subjective and hence nothing can be factually called as right or wrong.\n",
    "    \n",
    "### What is it that we want to understand?\n",
    "    The basic questions that pop up often are\n",
    "        1. Who is talking about the product?\n",
    "        2. What is that product?\n",
    "    As we seek answers to these questions, the curiosity leads to few more questions-\n",
    "        1. What is the opinion?\n",
    "        2. What is the background under which this opinion was expressed?\n",
    "        3. Is it good for the product? Is it positive or negative?\n",
    "\n",
    "### How easy is this task of opinion mining?\n",
    "    Well, sometimes we readily have access to the information like who is talking about the product. But at times, we may only have the text passage and then the opinion holder and the target product is hidden in the text(may be the passage refers indirectly to the a government personnel and the opinion holder is from opponent government party!). It would certainly involve information deduction from the passage. \n",
    "    Also, the problem may get a little complex when opinion provider is a group than an individual, target may be someone else's opinion or a set of products than a single entity, or the opinion text or context is highly complex.\n",
    "    \n",
    "## Why Opinion Mining?\n",
    "    Some reasons that intuitively come forward are-\n",
    "        1. To make better and improved decisions\n",
    "        2. To understand people\n",
    "        3. To improve and make targeted advertising\n",
    "        4. For Business Intelligence\n",
    "        5. For Market Research\n",
    "        6. For any other research...\n",
    "        \n",
    "## What is Sentiment Analysis?\n",
    "    Basically, its a classification problem! Very often, we already know the opinion bearer, opinion target/product, the context of the opinion and the context. Only thing left is analysing the sentiments!\n",
    "    So, the input is definitely the text data and output is a sentiment. However, there could be 2 types of analysis here. \n",
    "        a. Polarity Analysis - positive, negative, neutral or rank ordered categories like 1, 2, 3, 4\n",
    "        b. Emotion Analysis - sad, happy, angry, scared, disgusted\n",
    "    But either way, it is sentiment classification problem.\n",
    "    \n",
    "## How do we do this analysis?\n",
    "    Feature Identification comes here for rescue. It is the most complex step and identification of right features can make a huge difference. It is said that Natural Language Processing is an amazing tool for identifying right features but could lead to overfitting.\n",
    "    \n",
    "    Some of the features that are commonly used are -\n",
    "        1. Character n-grams - n could be any number that analyst finds relevant. This represents characters allowed for analysis\n",
    "        2. Word n-grams - This represents total words allowed\n",
    "        3. Parts of speech(POS) tag n-grams - This refers to adjective, noun, verbs etc. allowed\n",
    "        4. Word classes - It could be Syntactics like POS tags, semantics like thesaurus, or some other word clusters\n",
    "        5. Word Patterns - It represents frequently used word patterns\n",
    "        6. Sentence Patterns - They are specific set of repeating sentences\n",
    "        \n",
    "    Choosing the right feature could be a tough call! Just to elaborate, selection of text features depends on the purpose of your mining task. For example, if a data analyst aims to classify text as positive or negative, unigram (1-gram) word feature would be a bad feature. Lets say there are 2 sentences, 'I love my iPhone.' and 'I don't love my iPhone as much as I love my MacBook.' If the unigram feature was selected and 'love' was the unigram in consideration, we would end up classifying both the sentences as positive for iPhone with respect to postiveness indicated by the unigram 'love' even when the second sentence is a negative one.\n",
    "    \n",
    "## How do we define machine learning process for Sentiment Analysis?\n",
    "    The steps are similar to those of the rest of the machine learning problems.\n",
    "    A. Select a set of features that as an analyst & domain expert you believe are appropriate\n",
    "    B. Train the features on your data\n",
    "    C. Validate the features on new data and modify the model features based on the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Reviews - Sentiment Analysis\n",
    "Let us now consider a movie review sentiment analysis use case in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ IMPORT DATA AND EXPLORE #########################\n",
    "\n",
    "# Importing nltk & random package\n",
    "import nltk\n",
    "import random\n",
    "# Imporing the dataset & stopwords corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating documents list which stores file name and its category as pos or neg\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "# we randomly shuffle the documents before creating training and testing datasets\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomly chosing 40th document to see its content\n",
    "# print(documents[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing categories\n",
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listing unique file ids\n",
    "# movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding out number of categories\n",
    "len(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding out number of distinct file ids\n",
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we define a function to find out useful words such that they are not stop words from the English language\n",
    "# We also segregate these words and create our own dictionary to store all such words in a data object called dict\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we create a new vector for movies having negative reviews\n",
    "# the for loop is for scanning every fileid that belongs to neg category\n",
    "# iteratively for all neg file ids words are scanned & stored into 'words'\n",
    "# all such words are then appended into the 'neg_reviews' vector\n",
    "# As a result, we get a list of all words which are used in negative reviews\n",
    "# We use the user defined function 'create_word_features' for storing only useful words and not stop words\n",
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    neg_reviews.append((create_word_features(words), \"negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just as neg_reviews, we create a vector to store words from positive reviews\n",
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append((create_word_features(words), \"positive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# To verify the vector contents we print the length of neg_reviews; \n",
    "# which should come out to be 1000 as there are exactly 1000 neg reviews\n",
    "print(len(neg_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"': True,\n",
       " \"'\": True,\n",
       " '(': True,\n",
       " ')': True,\n",
       " ',': True,\n",
       " '.': True,\n",
       " 'across': True,\n",
       " 'acting': True,\n",
       " 'action': True,\n",
       " 'another': True,\n",
       " 'around': True,\n",
       " 'average': True,\n",
       " 'back': True,\n",
       " 'baldwin': True,\n",
       " 'bastard': True,\n",
       " 'big': True,\n",
       " 'body': True,\n",
       " 'brain': True,\n",
       " 'bringing': True,\n",
       " 'brother': True,\n",
       " 'bug': True,\n",
       " 'cgi': True,\n",
       " 'chase': True,\n",
       " 'comes': True,\n",
       " 'course': True,\n",
       " 'crew': True,\n",
       " 'curtis': True,\n",
       " 'damn': True,\n",
       " 'deserted': True,\n",
       " 'design': True,\n",
       " 'donald': True,\n",
       " 'drunkenly': True,\n",
       " 'empty': True,\n",
       " 'even': True,\n",
       " 'feels': True,\n",
       " 'flash': True,\n",
       " 'flashy': True,\n",
       " 'get': True,\n",
       " 'going': True,\n",
       " 'good': True,\n",
       " 'gore': True,\n",
       " 'got': True,\n",
       " 'h20': True,\n",
       " 'halloween': True,\n",
       " 'happy': True,\n",
       " 'head': True,\n",
       " 'hey': True,\n",
       " 'hit': True,\n",
       " 'jamie': True,\n",
       " 'kick': True,\n",
       " 'know': True,\n",
       " 'lee': True,\n",
       " 'let': True,\n",
       " 'like': True,\n",
       " 'likely': True,\n",
       " 'likes': True,\n",
       " 'little': True,\n",
       " 'middle': True,\n",
       " 'mir': True,\n",
       " 'movie': True,\n",
       " 'much': True,\n",
       " 'nowhere': True,\n",
       " 'occasional': True,\n",
       " 'origin': True,\n",
       " 'otherwise': True,\n",
       " 'parts': True,\n",
       " 'people': True,\n",
       " 'picking': True,\n",
       " 'pink': True,\n",
       " 'power': True,\n",
       " 'pretty': True,\n",
       " 'quick': True,\n",
       " 'real': True,\n",
       " 'really': True,\n",
       " 'regarding': True,\n",
       " 'review': True,\n",
       " 'robot': True,\n",
       " 'robots': True,\n",
       " 'russian': True,\n",
       " 'schnazzy': True,\n",
       " 'sequences': True,\n",
       " 'ship': True,\n",
       " 'shot': True,\n",
       " 'someone': True,\n",
       " 'stan': True,\n",
       " 'star': True,\n",
       " 'starring': True,\n",
       " 'start': True,\n",
       " 'still': True,\n",
       " 'story': True,\n",
       " 'strangeness': True,\n",
       " 'stumbling': True,\n",
       " 'substance': True,\n",
       " 'sunken': True,\n",
       " 'sutherland': True,\n",
       " 'tech': True,\n",
       " 'thing': True,\n",
       " 'throughout': True,\n",
       " 'time': True,\n",
       " 'took': True,\n",
       " 'tugboat': True,\n",
       " 'turn': True,\n",
       " 'virus': True,\n",
       " 'wasted': True,\n",
       " 'well': True,\n",
       " 'william': True,\n",
       " 'winston': True,\n",
       " 'within': True,\n",
       " 'work': True,\n",
       " 'y2k': True}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the contents of neg_reviews, we run following command and we expect 1000 lists of lists of words\n",
    "neg_review_sample = [item[0] for item in neg_reviews]\n",
    "neg_review_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 500\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing dataset\n",
    "train_set = neg_reviews[:750] + pos_reviews[:750]\n",
    "test_set =  neg_reviews[750:] + pos_reviews[750:]\n",
    "print(len(train_set),  len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing necessary packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content</td>\n",
       "      <td>polarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the happy bastard's quick movie review damn th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  polarity\n",
       "0                                            content  polarity\n",
       "1  plot : two teen couples go to a church party ,...         0\n",
       "2  the happy bastard's quick movie review damn th...         0\n",
       "3  it is movies like these that make a jaded movi...         0\n",
       "4   \" quest for camelot \" is warner bros . ' firs...         0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting up working directory\n",
    "os.getcwd()\n",
    "os.chdir(\"/Users/dhanashreepokale/Downloads/Data Mining/tm/data\")\n",
    "df = pd.read_csv(\"movie_reviews_n.txt\", sep='\\t',names=['content','polarity'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TFIDF Vectorizer\n",
    "stopset = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase = True, strip_accents = 'ascii', stop_words= stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in this case, our dependent variable witll be polarity as 0(din't like the movie) or 1 (liked the movie)\n",
    "y=df.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert df.tsv from text to features\n",
    "X= vectorizer.fit_transform(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001,)\n",
      "(2001, 39516)\n"
     ]
    }
   ],
   "source": [
    "#observationsx unique words\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test Train Split as usual\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we will train a naive bayes classifier\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################## PIPELINE #######################\n",
    "\n",
    "#importing required packages\n",
    "import sklearn.pipeline\n",
    "from sklearn import ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specifying feature selection technique and its parameters\n",
    "select = sklearn.feature_selection.SelectKBest(k=100)\n",
    "#specifying the classifier\n",
    "clf = sklearn.ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a steps object to store above mentioned techniques\n",
    "steps = [('feature_selection', select),\n",
    "        ('random_forest', clf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using pipeline for tightening up the steps code\n",
    "pipeline = sklearn.pipeline.Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################## sampling #######################\n",
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.80      0.74       332\n",
      "          1       0.76      0.64      0.70       329\n",
      "\n",
      "avg / total       0.73      0.72      0.72       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################## MODEL FITTING & PREDICTION REPORT #######################\n",
    "### fit your pipeline on X_train and y_train\n",
    "pipeline.fit( X_train, y_train )\n",
    "### call pipeline.predict() on your X_test data to make a set of test predictions\n",
    "y_prediction = pipeline.predict( X_test )\n",
    "### test your predictions using sklearn.classification_report()\n",
    "report = sklearn.metrics.classification_report( y_test, y_prediction )\n",
    "### and print the report\n",
    "print(report)\n",
    "warnings.filterwarnings('ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
